services:
  raft:
    build:
      context: .
      dockerfile: Dockerfile
    image: raft-pixi:latest
    container_name: raft-optical-flow
    
    # GPU configuration
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    
    # Environment variables
    environment:
      - CUDA_VISIBLE_DEVICES=all
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    
    # Volume mounts
    volumes:
      - ./datasets:/workspace/raft/datasets
      - ./outputs:/workspace/raft/outputs
      - ./runs:/workspace/raft/runs
      - ./models:/workspace/raft/models
      - raft-cache:/root/.cache
    
    # Working directory
    working_dir: /workspace/raft
    
    # Keep container running
    stdin_open: true
    tty: true
    
    # Restart policy
    restart: unless-stopped

  # Optional: Tensorboard service for monitoring training
  tensorboard:
    image: tensorflow/tensorflow:latest
    container_name: raft-tensorboard
    command: tensorboard --logdir=/runs --host=0.0.0.0 --port=6006
    ports:
      - "6006:6006"
    volumes:
      - ./runs:/runs
    restart: unless-stopped
    profiles:
      - monitoring

volumes:
  raft-cache:
    driver: local
